Inference parameters for foundation models
---------------------------------------------
----------------------------------------------
- LLMs generate one word at a time.
- The LLM model can access all possible words when generating the next word. Each possible word is associated with a probability, representing how likely the word appears after the previous word. The sum of the probabilities of all possible words equals 1.0 (100%). The potential words are sorted by their probabilities of appearance.
- The LLM model picks a word from all possible words mentioned above. The decision is affected by the inference parameters.

Temperature, Top P, Max Tokens, Stop Sequences
----------------------------------------------
- It should be noted that different foundation models might treat the same parameter differently.
- Amazon Titan models, Anthropic Claude models, AI21 Labs Jurrasic-2 models, and Cohere Command models commonly support these parameters.
- The actual names used in different models might be slightly different. However, the **Converse API** provides a consistent interface to all foundation models despite the differences in their implementation details.
```
import json
import boto3

prompt = """
Classify text into three categories - positive, neutral, and negative. The output should be a properly formatted JSON string.

{
    "sentiment": <sentiment></sentiment>
}

Only output the JSON string, nothing else.

==== TEXT ====

Can I please get someone to assist me? I have been waiting for 2 minutes on the line.
"""

model_id = 'ai21.j2-ultra'
bedrock = boto3.client(service_name='bedrock-runtime')
message = {"role": "user", "content": [{"text": prompt}]}          
messages = [message]
inference_config = {"temperature": 1.0, "topP": 1.0, "maxTokens": 200, "stopSequences": []}
response = bedrock.converse(                # converse method()
    modelId=model_id,
    messages=messages,
    inferenceConfig=inference_config
)
content = response['output']['message']['content']
for item in content:
    print(item['text'])


```
- Temperature - The Temperature value ranges from 0 to 1. When you set the temperature closer to zero, the model tends to select the higher-probability words. When you set the temperature further away from zero, the model may select a lower-probability word.
- Top P - Top P defines a cut-off based on the sum of probabilities of the potential choices. When you set Top P to 1.0, all possible words are counted towards potential choices. When you set Top P to below 1.0, only the top words are counted towards potential choices, and the sum of probabilities of these words must be <= Top P. As an over-simplified example, letâ€™s assume that all possible words include {A, B, C, D} and their probabilities are {0.4, 0.3, 0.2, 0.1}. If we set Top P to 0.9, then the LLM model only considers words in {A, B, C}. If we set Top P to 0.8, then the LLM model only considers words in {A, B}. If we set Top P to 0.5, then the LLM model only considers words in {A}.
- Max Token - Max Token defines the maximum number of tokens in the generated response.
- Stop Sequences - If the content generated by the LLM model contains the stop sequences, even if the number of tokens generated is smaller than Max Token, the LLM model will stop generating more tokens.

- When generating the next word, we can think of the following:
```
There is a list of all possible words, sorted by their probabilities.
The LLM model produces a short list using Top P.
The LLM model picks a word from the short list using Temperature.
The LLM model stops generating the next word when Max Token is achieved.
If the LLM model generates a stop sequence, the LLM model also stops generating the next word.
```
