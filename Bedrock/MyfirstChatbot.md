# Create your own Chatbot using AWS Bedrock and AWS services
### What does the future hold for generative AI?
 - Agents
 - Multimodal
 - Multiple Models
 - Regulations
         

Inference parameters for foundation models
---------------------------------------------
----------------------------------------------
- LLMs generate one word at a time.
- The LLM model can access all possible words when generating the next word. Each possible word is associated with a probability, representing how likely the word appears after the previous word. The sum of the probabilities of all possible words equals 1.0 (100%). The potential words are sorted by their probabilities of appearance.
- The LLM model picks a word from all possible words mentioned above. The decision is affected by the inference parameters.

Temperature, Top P, Max Tokens, Stop Sequences
----------------------------------------------
- It should be noted that different foundation models might treat the same parameter differently.
- Amazon Titan models, Anthropic Claude models, AI21 Labs Jurrasic-2 models, and Cohere Command models commonly support these parameters.
- The actual names used in different models might be slightly different. However, the **Converse API** provides a consistent interface to all foundation models despite the differences in their implementation details.
```
import json
import boto3

prompt = """
Classify text into three categories - positive, neutral, and negative. The output should be a properly formatted JSON string.

{
    "sentiment": <sentiment></sentiment>
}

Only output the JSON string, nothing else.

==== TEXT ====

Can I please get someone to assist me? I have been waiting for 2 minutes on the line.
"""

model_id = 'ai21.j2-ultra'
bedrock = boto3.client(service_name='bedrock-runtime')
message = {"role": "user", "content": [{"text": prompt}]}          
messages = [message]
inference_config = {"temperature": 1.0, "topP": 1.0, "maxTokens": 200, "stopSequences": []}
response = bedrock.converse(                # converse method()
    modelId=model_id,
    messages=messages,
    inferenceConfig=inference_config
)
content = response['output']['message']['content']
for item in content:
    print(item['text'])


```
- Temperature - The Temperature value ranges from 0 to 1. When you set the temperature closer to zero, the model tends to select the higher-probability words. When you set the temperature further away from zero, the model may select a lower-probability word.
- Top P - Top P defines a cut-off based on the sum of probabilities of the potential choices. When you set Top P to 1.0, all possible words are counted towards potential choices. When you set Top P to below 1.0, only the top words are counted towards potential choices, and the sum of probabilities of these words must be <= Top P. As an over-simplified example, let’s assume that all possible words include {A, B, C, D} and their probabilities are {0.4, 0.3, 0.2, 0.1}. If we set Top P to 0.9, then the LLM model only considers words in {A, B, C}. If we set Top P to 0.8, then the LLM model only considers words in {A, B}. If we set Top P to 0.5, then the LLM model only considers words in {A}.
- Max Token - Max Token defines the maximum number of tokens in the generated response.
- Stop Sequences - If the content generated by the LLM model contains the stop sequences, even if the number of tokens generated is smaller than Max Token, the LLM model will stop generating more tokens.

- When generating the next word, we can think of the following:
```
There is a list of all possible words, sorted by their probabilities.
The LLM model produces a short list using Top P.
The LLM model picks a word from the short list using Temperature.
The LLM model stops generating the next word when Max Token is achieved.
If the LLM model generates a stop sequence, the LLM model also stops generating the next word.
```
TOP K
--------
Top K is a parameter supported by Anthropic Claude models and Cohere Command models. Top K defines the cut off where the LLM model no longer selects the words. As an over-simplified example, let’s assume that all possible words include {A, B, C, D, E, F, G} and their probabilities are {0.25, 0.20, 0.15, 0.14, 0.13, 0.07, 0.06}. If we set Top K to 50 then the LLM model considers all possible options. If we set Top K to 5 then the LLM model only considers the top 5 possible words {A, B, C, D, E}.

```
import json
import boto3

prompt = """
Describe the color of the sky in less than 10 words.
"""

model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'
bedrock = boto3.client(service_name='bedrock-runtime')
message = {"role": "user", "content": [{"text": prompt}]}
messages = [message]
inference_config = {"temperature": 1.0, "topP": 1.0, "maxTokens": 200, "stopSequences": []}      # topP
additional_model_fields = {"top_k": 200}
response = bedrock.converse(
    modelId=model_id,
    messages=messages,
    inferenceConfig=inference_config,
    additionalModelRequestFields=additional_model_fields
)
content = response['output']['message']['content']
for item in content:
    print(item['text'])

```
Prompt Engineering Basics
-----------------------------
- Zero-Shot Prompting
- Few-Shot Prompting
- Chain-of-Thoughts Prompting
```
- A response without any examples on the expected response. In prompt engineering, this is referred to as zero-shot prompting.[Ex - above script]
    - Run this example multiple times with the same model. Each time you get a creative answer. If you are expecting a specific kind of answer, the generated text might not be what you are looking for. Think of this as someone asking you to do something without clear instructions. It is hard to guess what the other person really wants.
- Few-shot prompting means you ask for a response with a few examples on the expected response. Ex :
prompt = """
A dog has 4 legs.
A sheep has 4 legs.
A spider
"""
- Chain-of-thought (CoT) prompting teaches LLMs to reason through intermediate steps, usually assisted by examples (few-shot prompting).
 - Assuming that we want to infer the sentiment that is opposite to the sentiment associate with a sentence, we wish to achieve something like the following. This proves to be hard for some models.

2
3
4
5
6
prompt = """
This is awesome! // Negative
This is bad! // Positive
Wow that movie was rad! // Negative
What a horrible show! //
"""

```
Conversational Chatbot
---------------------------
You are now able to ask a question and receives the generated text. When you ask a follow up question, the generated text seems to be unrelated to the previous question and answer. Each question seems to be treated separately - the foundation models do not seem to remember what has previously been asked and answered.

For foundation models to remember what has been asked and answered, it requires something similar to the concept of a session on the server side. Considering the size of the conversation data, this is something very expensive to offer, especially in a multi-tenant environment.

A workaround is, instead of asking the service to keep the chat history, the customer keeps chat history on the client side. When asking a follow up question, the client sends the chat history as part of the prompt so that the LLM models know what has been talked about.

```
# Note that this example uses the old InvokeModel API instead of the new Converse API. I intentionally do this to show how "conversation history" looks like in plain text.
import json
import boto3

bedrock = boto3.client(service_name='bedrock-runtime')
model_id = 'ai21.j2-ultra'
prompt = """
Human: How are you?
"""
input = {
    'prompt': prompt, 
    'maxTokens': 1024,
    'temperature': 1.0,
    'topP': 1.0,
    'stopSequences': []
}
body = json.dumps(input)
response = bedrock.invoke_model(body=body, modelId=model_id)    # invoke
response_body = json.loads(response.get('body').read())
completions = response_body['completions']
for part in completions:
    print(part['data']['text'])

```
Critical to designing your Conversational Chatbot
--------------------------------------------------
Obviously, as the conversation goes on and on, the chat history becomes bigger and bigger. This means the number of tokens in the input text grows over time, making the ongoing conversation more and more expensive. A good chatbot needs to achieve a careful balance between cost and conversation quality. By including only the most recent N dialogs in the prompt, we avoid over spending and the foundation model still produces generated text with good quality.

An important consideration with feeding in previous chat history into the model is the ‘**context size**’. Each foundation model has a defined ‘context size’ which is the maximum number of tokens that a model accepts as the input. This is different depending on the model and can range from 8,000 (Amazon Titan) to 100,000 (Claude V2 100k). You can think of this as the maximum length of an input prompt. In the case of a conversation chatbot, the size of the chat history you feed to the foundation needs to be smaller than the 'context size'.

```
# In the previous example, we use the InvokeModel API to demonstrate the flow of a conversation, but have to use stop sequences to prevent the model from generating the human part of the conversation.

import json
import boto3

model_id = 'amazon.titan-text-lite-v1'
bedrock = boto3.client(service_name='bedrock-runtime')
messages = [
    {"role": "user",      "content": [{"text": "How are you?"}]},
    {"role": "assistant", "content": [{"text": "I am well, thank you for asking! I hope you are well, too."}]},
    {"role": "user",      "content": [{"text": "I can't find my cat."}]},
    {"role": "assistant", "content": [{"text": "Did the cat go missing recently?"}]},
    {"role": "user",      "content": [{"text": "I saw it in the garden yesterday."}]}
]
inference_config = {"temperature": 1.0, "topP": 1.0, "maxTokens": 200, "stopSequences": []}   #,maxTokens,stopSquences
response = bedrock.converse(
    modelId=model_id,
    messages=messages,
    inferenceConfig=inference_config
)
content = response['output']['message']['content']
for item in content:
    print(item['text'])

```
Conversational Chatbot
-------------------------------
Now we will build a conversation chatbot service, which allows end users to chat with it via a web UI. This is a serverless application with the following workflow:

* The end user makes an HTTP request to an API Gateway.
* The API Gateway passes the request parameters to a Lambda function.
* The Lambda function extracts the prompt from the request parameters, invokes a foundation model in Amazon Bedrock.
* The Lambda function returns the response to API Gateway.
* API Gateway returns the response to the end user.


<img width="800" alt="image" src="https://github.com/user-attachments/assets/79431721-db65-431b-a613-5a94f4134366">
 
 - source :AWS public workshop

The HTML Code
--------------------
```
<HTML>
    <HEAD>
        <META http-equiv='Content-Type' content='text/html; charset=UTF-8'>
        <TITLE>Amazon Bedrock Development Tutorial</TITLE>
    </HEAD>
    <BODY>
        <textarea id="chat_history" name="chat_history" rows="40" cols="120" disabled></textarea>
        <P>Enter your message below:</P>
        <textarea id="chat_entry" name="chat_entry" rows="3" cols="100"></textarea>
        <button type="button" onclick="add()">&nbsp;&nbsp;Send&nbsp;&nbsp;</button> 
    </BODY>
<HTML>

<script>
var chat_depth   = 20;
var chat_history = [];
var chat_updated = false;
var conn_ongoing = false;
setInterval(doChat, 2000);

function add() {
    // Get chat entry
    const msg = document.getElementById("chat_entry").value.trim();
    // Make sure that it is not empty
    if (msg !== "") {
        // This is an entry from human
        const entry = {"role": "user", "content": [{"text": msg}]};
        // Add this to the chat history array
        chat_history.push(entry);
        chat_updated = true;
        // Add this to the chat history pane
        document.getElementById("chat_history").value += "Human: " + msg + "\n";
    }
    // Clear out the chat entry
    document.getElementById("chat_entry").value = "";
}

function doChat() {
    // Only connect to the chatbot when there is an update by the user.
    if (chat_updated) {
        // Only connect to the chatbot when there is no ongoing connection.
        if (!conn_ongoing) {
            // Only work on the last N messages in the chat history
            var start = 0;
            if (chat_history.length > chat_depth) {
                start = chat_history.length - chat_depth;
            }
            var messages = [];
            for (let i = start; i < chat_history.length; i++) {
                messages.push(chat_history[i]);
            }
            
            // Connect to the chatbot
            var xhr = new XMLHttpRequest();
            xhr.open('POST', '/demo', true);
            xhr.onload = function () {
                // Extract the chatbot response
                var msg = this.responseText.trim();
                var entry = {"role": "assistant", "content": [{"text": msg}]};
                // Add this to the chat history array
                chat_history.push(entry);
                // Add this to the chat history pane
                document.getElementById("chat_history").value += "Assistant: " + msg + "\n";
                // Set the conn_ongoing flag to false
                conn_ongoing = false;
            };
            // Set the chat_updated off and conn_ongoing flags on and send the request.
            chat_updated = false;
            conn_ongoing = true;
            xhr.send(JSON.stringify({"messages": messages}));
        }
    }
} 
</script>

```
Python File 
-------------
```
# On a high level, this Python script does the following things:
"""
- For all GET requests, we load the HTML content from index.html and return it to the requester.
- For all POST requests, we extract the body from the request, use it as the prompt to invoke a foundation model, then return the response from the foundation model to the requester.
- For all other requests, we simply return an OK to the requester.
"""

import os
import json
import boto3

model_id = os.environ.get('model_id')
bedrock = boto3.client(service_name='bedrock-runtime')

def lambda_handler(event, context):
    if (event['httpMethod'] == 'GET'):
        output = load_html()
        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'text/html'},
            'body': output
        }
    elif (event['httpMethod'] == "POST"):
        body = json.loads(event['body'])
        messages = body['messages']
        print(messages)
        output = chat(messages)
        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'text/html'},
            'body': output
        }
    else:
         return {
            'statusCode': 200,
            'headers': {'Content-Type': 'text/html'},
            'body': "OK"
        }

def load_html():
    html = ''
    with open('index.html', 'r') as file:
        html = file.read()
    return html

def chat(messages):
    inference_config = {'temperature': 1.0, 'topP': 1.0, 'maxTokens': 1024}
    response = bedrock.converse(
        modelId=model_id,
        messages=messages,
        inferenceConfig=inference_config
    )
    content = response['output']['message']['content']
    output = ''
    for item in content:
        output = output + item['text'] + '\n'
    return output

```
Working with Embedding
----------------------------
- various ways to calculate vector similarity (Euclidean distance, dot product similarity, cosine similarity)
- We demonstrate some common use cases of embedding, including search and recommendation, classification, clustering, and outlier detection.
- We introduce a few ways to store and search vector data, including flat files, **pgvector, OpenSearch Serverless, and Pinecone.**
- we will use Amazon Titan Embeddings G1 - Text (Embedding) model in the Amazon Bedrock console

#### What is Embedding ?
---------------------------

The academic definition of embedding is translating high-dimensional vectors into a relatively low-dimensional space. You might know each and every word in this sentence but still have no idea about the whole sentence. We can think of embedding as converting natural language into a sequence of numbers, with the input being a piece of text and the output being a vector. In other words, the vector is a numerical representation of the text, making it easy to perform all kinds of complex computations in AI/ML.

```
import json
import boto3

bedrock = boto3.client(
    service_name='bedrock-runtime'
)

modelId = 'amazon.titan-embed-text-v1'      # Amazon Titan Embeddings G1 - Text (Embedding) model 
accept = 'application/json'
contentType = 'application/json'
prompt = """
hello
"""
input = {
        'inputText': prompt
    }
body=json.dumps(input)
response = bedrock.invoke_model(
    body=body, modelId=modelId, accept=accept,contentType=contentType)
response_body = json.loads(response.get('body').read())
embedding = response_body['embedding']
print(embedding)

```
With an embedding model, the model translates a piece of text into a fixed length vector. The vector conveys the same message as the original text and can be understood by the model. By using fixed length vectors, it is easier for the model to perform computations.

```
# Cosine Similarity is another commonly used method to measure similarity. In linear algebra, cosine similarity is the cosine of the angle between two vectors. That is, it is the dot product of the vectors divided by the product of their lengths. Here is the sample code to calculate the cosine similarity between “hello” and “good day”.

import json
import boto3
from numpy import dot
from numpy.linalg import norm

def get_embedding(bedrock, text):
    modelId = 'amazon.titan-embed-text-v1'
    accept = 'application/json'
    contentType = 'application/json'
    input = {
            'inputText': text
        }
    body=json.dumps(input)
    response = bedrock.invoke_model(
        body=body, modelId=modelId, accept=accept,contentType=contentType)
    response_body = json.loads(response.get('body').read())
    embedding = response_body['embedding']
    return embedding

def calculate_cousin_similarity(v1, v2):
    similarity = dot(v1, v2)/(norm(v1)*norm(v2))
    return similarity

# main function
bedrock = boto3.client(
    service_name='bedrock-runtime'
)
text1 = 'hello'
text2 = 'good day'
v1 = get_embedding(bedrock, text1)
v2 = get_embedding(bedrock, text2)
similarity = calculate_cousin_similarity(v1, v2)
print(similarity)

```
#### What is Vector Databases ?
---------------------------------------
When you have a large data set with each entry in the data set represented by its embedding, you will need a way to store the data set along with the embedding for efficient search and retrieval. Vector databases are purpose-built database systems specialized in storing and searching vector data.

#### pgvector

PostgreSQL has a pgvector extension for vector similarity search. The advantage of pgvector is most programming language has the library to connect to PostgreSQL. Also, customers can use their favorite SQL client to work with the data during prototyping and troubleshooting.

Amazon RDS for PostgreSQL (version 15.3 or later) supports pgvector.


```
# This will create a table called dataset in the RDS instance.
import json
import boto3
import psycopg2
from botocore.exceptions import ClientError

def get_secrets():
    client = boto3.client(
        service_name='secretsmanager',
    )
    try:
        get_secret_value_response = client.get_secret_value(
            SecretId=secret_name
        )
    except ClientError as e:
        raise e
    secrets = json.loads(get_secret_value_response['SecretString'])
    return secrets
    
def load_dataset(filename):
    dataset = []
    with open(filename) as file:
        for line in file:
            dataset.append(json.loads(line))
    return dataset
    
# main function
secrets = get_secrets()
conn = psycopg2.connect(
    host=secrets['db_hostname'],
    port=secrets['db_hostport'],
    user=secrets['db_username'],
    password=secrets['db_password'],
    database=secrets['db_database']
)
cursor = conn.cursor()
cursor.execute('CREATE EXTENSION vector')
cursor.execute('CREATE TABLE dataset (id SERIAL, content TEXT, embedding VECTOR(1536))')
conn.commit()
print('Table created.')

```
```
# This code loads the dataset we created previously into your database.
import json
import boto3
import psycopg2
from botocore.exceptions import ClientError

def get_secrets():
    client = boto3.client(
        service_name='secretsmanager',
    )
    try:
        get_secret_value_response = client.get_secret_value(
            SecretId=secret_name
        )
    except ClientError as e:
        raise e
    secrets = json.loads(get_secret_value_response['SecretString'])
    return secrets
    
def load_dataset(filename):
    dataset = []
    with open(filename) as file:
        for line in file:
            dataset.append(json.loads(line))
    return dataset
    
# main function
secrets = get_secrets()
conn = psycopg2.connect(
    host=secrets['db_hostname'],
    port=secrets['db_hostport'],
    user=secrets['db_username'],
    password=secrets['db_password'],
    database=secrets['db_database']
)
cursor = conn.cursor()
# populate the data into the database
sql = 'INSERT INTO dataset (content, embedding) VALUES(%s, %s)'
dataset = load_dataset('dataset.json')
for item in dataset:
    cursor.execute(sql, (item['text'], item['embedding']))
conn.commit()
print('Loaded data into table.')

```

#### OpenSearch Serverless Vector Search
---------------------------------------------
Amazon OpenSearch Serverless is an on-demand serverless configuration for Amazon OpenSearch Service. Serverless removes the operational complexities of provisioning, configuring, and tuning your OpenSearch clusters. An OpenSearch Serverless collection is a group of OpenSearch indexes that work together to support a specific workload or use case. Collections are easier to use than self-managed OpenSearch clusters, which require manual provisioning.

#### Pinecone
-------------------
 
Pinecone is a fully managed vector storage and search service offered by a third-party. You can get a starter account for free for learning and testing purposes. After you register, you get an API key to use in your code.

Retrieval Augmented Generation (RAG)
----------------------------------------
Convert the prompt (question text) into embedding.
(R) Retrieve N most relevant entries from the knowledge base. This is treated as the context of the conversation.
(A) Augment the prompt with the context by prepending the context to the question text. This end result is the context aware prompt.
(G) Generate the answer by feeding the context aware prompt to the foundation model.ers:


- We will now integrate Retrieval Augmented Generation (RAG) capability into the chatbot.

Knowledge Bases and Agents
---------------------------
In the previous secion, we introduce the concept of embedding and demonstrate how the combination of embedding and vector databases makes it possible to perform retrieval augmented generation (RAG). The user still needs to create the vector database, ingest data into the vector databse, retrieve data from the vector database, and construct the prompt to perform RAG. In this section, we demonstrate how Amazon Bedrock natively supports knowledge bases and RAG, making it easier to build applications with RAG capabilities.

#### Agents

FMs are capable of doing various tasks, such as summarization, content generation, etc. However, since foundation models (FMs) are trained on public and historical data, there are limitations on their capabilities. For example, FMs cannot answer questions like "what's the time now", and FMs cannot take actions on your behalf like making online orders. To extend the functionality of AI/ML applications beyond what FMs can do, developers need to build an ecosystem around FMs. Agents greatly help developers simplify these tasks.

Agents for Amazon Bedrock offers the ability to build and configure autonomous agents in AI/ML applications. An agent can complete actions based on organization data and user input. Agents orchestrate interactions between FMs, data sources, software applications, and user conversations. In addition, agents automatically makes API calls to take actions, or invoke knowledge bases to supplement information for these actions.




